{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NLI base results: BERTurk base (dbmdz/bert-base-turkish-cased)\n",
        "\n",
        "Loads [yilmazzey/sdp2-nli](https://huggingface.co/datasets/yilmazzey/sdp2-nli) (snli_tr_1_1, multinli_tr_1_1, trglue_mnli) and runs **test-only** evaluation with this model.\n",
        "\n",
        "**No prompts:** BERT NLI is sequence-pair classification (premise [SEP] hypothesis → label).\n",
        "\n",
        "**Splits:** Test only where available: snli → `test`; multinli → `validation_matched`/`validation_mismatched` (no test); trglue → `test_matched`/`test_mismatched`.\n",
        "\n",
        "**Metrics:** Accuracy, macro F1, per-class F1, confusion matrix (CSV + plot). Base BERTurk has a random head (~33% accuracy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "REPO_ID = \"yilmazzey/sdp2-nli\"\n",
        "CONFIGS = [\"snli_tr_1_1\", \"multinli_tr_1_1\", \"trglue_mnli\"]\n",
        "MODEL_ID = \"dbmdz/bert-base-turkish-cased\"\n",
        "NUM_LABELS = 3  # entailment, neutral, contradiction\n",
        "RESULTS_DIR = \"results\"\n",
        "BATCH_SIZE = 32\n",
        "EVAL_SPLITS = {\n",
        "    \"snli_tr_1_1\": [\"test\"],\n",
        "    \"multinli_tr_1_1\": [\"validation_matched\", \"validation_mismatched\"],\n",
        "    \"trglue_mnli\": [\"test_matched\", \"test_mismatched\"],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding\n",
        "\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    HAS_PLOT = True\n",
        "except ImportError:\n",
        "    HAS_PLOT = False\n",
        "\n",
        "LABEL_NAMES = [\"entailment\", \"neutral\", \"contradiction\"]\n",
        "\n",
        "# Reproducibility: fixed seed for random, numpy, torch (and cuda if available)\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading yilmazzey/sdp2-nli :: snli_tr_1_1 ...\n",
            "  splits: ['train', 'validation', 'test']\n",
            "Loading yilmazzey/sdp2-nli :: multinli_tr_1_1 ...\n",
            "  splits: ['train', 'validation_matched', 'validation_mismatched']\n",
            "Loading yilmazzey/sdp2-nli :: trglue_mnli ...\n",
            "  splits: ['train', 'validation_matched', 'validation_mismatched', 'test_matched', 'test_mismatched']\n"
          ]
        }
      ],
      "source": [
        "# Load all three dataset configs\n",
        "datasets = {}\n",
        "for cfg in CONFIGS:\n",
        "    print(f\"Loading {REPO_ID} :: {cfg} ...\")\n",
        "    datasets[cfg] = load_dataset(REPO_ID, cfg)\n",
        "    print(\"  splits:\", list(datasets[cfg].keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 1828.46it/s, Materializing param=bert.pooler.dense.weight]                               \n",
            "\u001b[1mBertForSequenceClassification LOAD REPORT\u001b[0m from: dbmdz/bert-base-turkish-cased\n",
            "Key                                        | Status     | \n",
            "-------------------------------------------+------------+-\n",
            "cls.predictions.bias                       | UNEXPECTED | \n",
            "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
            "cls.seq_relationship.bias                  | UNEXPECTED | \n",
            "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
            "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
            "cls.seq_relationship.weight                | UNEXPECTED | \n",
            "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
            "classifier.bias                            | MISSING    | \n",
            "classifier.weight                          | MISSING    | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
            "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID, num_labels=NUM_LABELS)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_fn(examples):\n",
        "    # Dynamic padding: no padding here; DataCollatorWithPadding in DataLoader\n",
        "    return tokenizer(\n",
        "        examples[\"premise\"],\n",
        "        examples[\"hypothesis\"],\n",
        "        truncation=True,\n",
        "        max_length=256,\n",
        "    )\n",
        "\n",
        "\n",
        "def run_inference(ds):\n",
        "    remove_cols = [c for c in ds.column_names if c not in (\"label\",)]\n",
        "    ds = ds.map(\n",
        "        tokenize_fn,\n",
        "        batched=True,\n",
        "        remove_columns=remove_cols,\n",
        "        desc=\"Tokenize\",\n",
        "    )\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    def collate_fn(examples):\n",
        "        labels = torch.tensor([ex[\"label\"] for ex in examples])\n",
        "        batch = data_collator([{k: v for k, v in ex.items() if k != \"label\"} for ex in examples])\n",
        "        batch[\"label\"] = labels\n",
        "        return batch\n",
        "\n",
        "    # batch_size=32; can lower to 16/8 if too slow on CPU\n",
        "    loader = torch.utils.data.DataLoader(ds, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "    preds_list, labels_list = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc=\"Inference\"):\n",
        "            if len(preds_list) == 0:\n",
        "                print(\"batch['input_ids'].shape:\", batch[\"input_ids\"].shape)\n",
        "            out = model(\n",
        "                input_ids=batch[\"input_ids\"].to(device),\n",
        "                attention_mask=batch[\"attention_mask\"].to(device),\n",
        "            )\n",
        "            preds_list.append(out.logits.argmax(-1).cpu().numpy())\n",
        "            labels_list.append(batch[\"label\"].numpy())\n",
        "    y_pred = np.concatenate(preds_list)\n",
        "    y_true = np.concatenate(labels_list)\n",
        "    return y_true, y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics(y_true, y_pred):\n",
        "    acc = float(accuracy_score(y_true, y_pred))\n",
        "    f1_macro = float(f1_score(y_true, y_pred, average=\"macro\", zero_division=0))\n",
        "    f1_per_class = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
        "    f1_per_class = {LABEL_NAMES[i]: float(f1_per_class[i]) for i in range(NUM_LABELS)}\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    out = {\"accuracy\": acc, \"f1_macro\": f1_macro, \"f1_per_class\": f1_per_class}\n",
        "    return out, cm\n",
        "\n",
        "\n",
        "def save_confusion_plot(cm, path):\n",
        "    if not HAS_PLOT:\n",
        "        return\n",
        "    fig, ax = plt.subplots(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=LABEL_NAMES, yticklabels=LABEL_NAMES, ax=ax)\n",
        "    ax.set_xlabel(\"Predicted\")\n",
        "    ax.set_ylabel(\"True\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path)\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating snli_tr_1_1 / test ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenize: 100%|██████████| 9824/9824 [00:00<00:00, 58216.28 examples/s]\n",
            "Inference:   0%|          | 0/307 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch['input_ids'].shape: torch.Size([32, 40])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference: 100%|██████████| 307/307 [01:32<00:00,  3.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  True label dist: {np.int64(1): 3219, np.int64(0): 3368, np.int64(2): 3237}\n",
            "  Pred label dist: {np.int64(2): 2323, np.int64(0): 4004, np.int64(1): 3497}\n",
            "  accuracy=0.3347, f1_macro=0.3288\n",
            "Evaluating multinli_tr_1_1 / validation_matched ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenize: 100%|██████████| 9809/9809 [00:00<00:00, 54379.68 examples/s]\n",
            "Inference:   0%|          | 0/307 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch['input_ids'].shape: torch.Size([32, 73])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference: 100%|██████████| 307/307 [02:27<00:00,  2.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  True label dist: {np.int64(1): 3123, np.int64(2): 3211, np.int64(0): 3475}\n",
            "  Pred label dist: {np.int64(1): 3748, np.int64(2): 4457, np.int64(0): 1604}\n",
            "  accuracy=0.3217, f1_macro=0.3110\n",
            "Evaluating multinli_tr_1_1 / validation_mismatched ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenize: 100%|██████████| 9825/9825 [00:00<00:00, 53023.94 examples/s]\n",
            "Inference:   0%|          | 0/308 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch['input_ids'].shape: torch.Size([32, 116])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference: 100%|██████████| 308/308 [02:33<00:00,  2.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  True label dist: {np.int64(2): 3240, np.int64(0): 3456, np.int64(1): 3129}\n",
            "  Pred label dist: {np.int64(1): 3518, np.int64(2): 5004, np.int64(0): 1303}\n",
            "  accuracy=0.3129, f1_macro=0.2933\n",
            "Evaluating trglue_mnli / test_matched ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenize: 100%|██████████| 9008/9008 [00:00<00:00, 50884.01 examples/s]\n",
            "Inference:   0%|          | 0/282 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch['input_ids'].shape: torch.Size([32, 73])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference: 100%|██████████| 282/282 [02:04<00:00,  2.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  True label dist: {np.int64(1): 3138, np.int64(2): 2946, np.int64(0): 2924}\n",
            "  Pred label dist: {np.int64(1): 3635, np.int64(2): 3905, np.int64(0): 1468}\n",
            "  accuracy=0.3323, f1_macro=0.3141\n",
            "Evaluating trglue_mnli / test_mismatched ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenize: 100%|██████████| 9217/9217 [00:00<00:00, 52341.35 examples/s]\n",
            "Inference:   0%|          | 0/289 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch['input_ids'].shape: torch.Size([32, 74])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference: 100%|██████████| 289/289 [02:04<00:00,  2.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  True label dist: {np.int64(1): 3043, np.int64(0): 3101, np.int64(2): 3073}\n",
            "  Pred label dist: {np.int64(2): 5031, np.int64(0): 1079, np.int64(1): 3107}\n",
            "  accuracy=0.3328, f1_macro=0.3010\n",
            "Saved results/metrics.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "Path(RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n",
        "all_metrics = {}\n",
        "\n",
        "for config_name in CONFIGS:\n",
        "    ds_dict = datasets[config_name]\n",
        "    split_names = EVAL_SPLITS[config_name]\n",
        "    all_metrics[config_name] = {}\n",
        "\n",
        "    for split_name in split_names:\n",
        "        if split_name not in ds_dict:\n",
        "            print(f\"  Skip {config_name}/{split_name} (missing)\")\n",
        "            continue\n",
        "        ds = ds_dict[split_name]\n",
        "        print(f\"Evaluating {config_name} / {split_name} ...\")\n",
        "        y_true, y_pred = run_inference(ds)\n",
        "        # Label distribution (true and predicted)\n",
        "        print(\"  True label dist:\", dict(Counter(y_true)))\n",
        "        print(\"  Pred label dist:\", dict(Counter(y_pred)))\n",
        "        metrics, cm = compute_metrics(y_true, y_pred)\n",
        "        all_metrics[config_name][split_name] = metrics\n",
        "\n",
        "        cm_path = Path(RESULTS_DIR) / f\"confusion_{config_name}_{split_name}.csv\"\n",
        "        np.savetxt(cm_path, cm, fmt=\"%d\", delimiter=\",\")\n",
        "        save_confusion_plot(cm, Path(RESULTS_DIR) / f\"confusion_{config_name}_{split_name}.png\")\n",
        "\n",
        "        print(f\"  accuracy={metrics['accuracy']:.4f}, f1_macro={metrics['f1_macro']:.4f}\")\n",
        "\n",
        "with open(Path(RESULTS_DIR) / \"metrics.json\", \"w\") as f:\n",
        "    json.dump(all_metrics, f, indent=2)\n",
        "print(f\"Saved {RESULTS_DIR}/metrics.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "snli_tr_1_1 / test: acc=0.3347, F1_macro=0.3288, F1_per_class={'entailment': 0.37710255018990774, 'neutral': 0.35318642048838594, 'contradiction': 0.25611510791366904}\n",
            "multinli_tr_1_1 / validation_matched: acc=0.3217, F1_macro=0.3110, F1_per_class={'entailment': 0.21894073636542627, 'neutral': 0.34463687963906275, 'contradiction': 0.36932707355242567}\n",
            "multinli_tr_1_1 / validation_mismatched: acc=0.3129, F1_macro=0.2933, F1_per_class={'entailment': 0.17062408068922041, 'neutral': 0.32014442605686777, 'contradiction': 0.3891314895681708}\n",
            "trglue_mnli / test_matched: acc=0.3323, F1_macro=0.3141, F1_per_class={'entailment': 0.17987249544626593, 'neutral': 0.36054923962793445, 'contradiction': 0.40198511166253104}\n",
            "trglue_mnli / test_mismatched: acc=0.3328, F1_macro=0.3010, F1_per_class={'entailment': 0.1291866028708134, 'neutral': 0.3460162601626016, 'contradiction': 0.42769002961500496}\n"
          ]
        }
      ],
      "source": [
        "# Summary: per config/split\n",
        "for config_name, splits in all_metrics.items():\n",
        "    for split_name, m in splits.items():\n",
        "        print(f\"{config_name} / {split_name}: acc={m['accuracy']:.4f}, F1_macro={m['f1_macro']:.4f}, F1_per_class={m['f1_per_class']}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
