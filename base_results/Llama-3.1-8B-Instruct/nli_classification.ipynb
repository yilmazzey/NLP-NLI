{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "llama31-title",
      "metadata": {},
      "source": [
        "# NLI base results: Llama-3.1-8B-Instruct (meta-llama/Llama-3.1-8B-Instruct) via Ollama on M4\n",
        "\n",
        "Loads [yilmazzey/sdp2-nli](https://huggingface.co/datasets/yilmazzey/sdp2-nli) (snli_tr_1_1, multinli_tr_1_1, trglue_mnli) and runs **test-only** zero-shot Turkish NLI evaluation with **Llama-3.1-8B-Instruct** via **Ollama** (no Hugging Face pipeline).\n",
        "\n",
        "Uses Llama 3.1 Instruct chat format (system + user turn; Ollama applies the template). Model is instructed to answer with exactly one word: entailment, neutral, or contradiction. Outputs parsed to 0=entailment, 1=neutral, 2=contradiction. Runs on Apple Silicon (M4) with pure Ollama (CPU/Metal), no CUDA, no quantization.\n",
        "\n",
        "**Splits:** snli → test; multinli → validation_matched/mismatched; trglue → test_matched/test_mismatched. **Metrics:** Accuracy, macro F1, per-class F1, confusion matrix (CSV + seaborn plot). Results saved to `./results/`. **Prerequisite:** `ollama pull llama3.1:8b` (or your Ollama model name for Llama-3.1-8B-Instruct)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "installs",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install ollama Python client if needed; standard libs for datasets/metrics/plots\n",
        "# !pip install -q ollama datasets scikit-learn tqdm matplotlib seaborn\n",
        "# If ollama is already installed via brew/pip, skip or run: pip install -q ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "imports-seed",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/zeynep_yilmaz/Desktop/Turkish_NLI/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on Apple Silicon (M4) / CPU — Ollama handles Metal.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "import re\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
        "from tqdm import tqdm\n",
        "import ollama\n",
        "\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    HAS_PLOT = True\n",
        "except ImportError:\n",
        "    HAS_PLOT = False\n",
        "\n",
        "LABEL_NAMES = [\"entailment\", \"neutral\", \"contradiction\"]\n",
        "\n",
        "# Device: M4 / Apple Silicon — no CUDA; Ollama uses Metal/CPU\n",
        "print(\"Running on Apple Silicon (M4) / CPU — Ollama handles Metal.\")\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "constants",
      "metadata": {},
      "outputs": [],
      "source": [
        "REPO_ID = \"yilmazzey/sdp2-nli\"\n",
        "CONFIGS = [\"snli_tr_1_1\", \"multinli_tr_1_1\", \"trglue_mnli\"]\n",
        "MODEL_ID = \"llama3.1:8b\"  # Ollama model name (e.g. after: ollama pull llama3.1:8b)\n",
        "NUM_LABELS = 3\n",
        "RESULTS_DIR = \"results\"\n",
        "BATCH_SIZE = 6  # Safe on M4 36GB RAM; use 4–8\n",
        "MAX_TOKENS = 10\n",
        "TEMPERATURE = 0.0\n",
        "TOP_P = 0.0\n",
        "EVAL_SPLITS = {\n",
        "    \"snli_tr_1_1\": [\"test\"],\n",
        "    \"multinli_tr_1_1\": [\"validation_matched\", \"validation_mismatched\"],\n",
        "    \"trglue_mnli\": [\"test_matched\", \"test_mismatched\"],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "load-datasets",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading yilmazzey/sdp2-nli :: snli_tr_1_1 ...\n",
            "  splits: ['train', 'validation', 'test']\n",
            "Loading yilmazzey/sdp2-nli :: multinli_tr_1_1 ...\n",
            "  splits: ['train', 'validation_matched', 'validation_mismatched']\n",
            "Loading yilmazzey/sdp2-nli :: trglue_mnli ...\n",
            "  splits: ['train', 'validation_matched', 'validation_mismatched', 'test_matched', 'test_mismatched']\n"
          ]
        }
      ],
      "source": [
        "# Load all three dataset configs (same as Turkish-Gemma-9b-T1)\n",
        "datasets = {}\n",
        "for cfg in CONFIGS:\n",
        "    print(f\"Loading {REPO_ID} :: {cfg} ...\")\n",
        "    datasets[cfg] = load_dataset(REPO_ID, cfg)\n",
        "    print(\"  splits:\", list(datasets[cfg].keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ollama-helper",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Llama 3.1 Instruct: system + user turn; Ollama applies <|start_header_id|>system/user/assistant<|end_header_id|> + <|eot_id|>\n",
        "SYSTEM_PROMPT = \"\"\"You are a natural language inference classifier. You must answer with exactly one word and nothing else: entailment, neutral, or contradiction. No explanation, no punctuation, no extra text. Only one of these three words.\"\"\"\n",
        "\n",
        "\n",
        "def nli_user_prompt(premise, hypothesis):\n",
        "    return f\"\"\"Premise: {premise}\n",
        "Hypothesis: {hypothesis}\n",
        "Does the premise entail, is neutral to, or contradict the hypothesis? Answer with only one word: entailment, neutral, or contradiction.\"\"\"\n",
        "\n",
        "\n",
        "def ollama_chat_single(user_content: str) -> str:\n",
        "    \"\"\"Llama 3.1 chat: system + user turn. Returns assistant message content only.\"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": user_content},\n",
        "    ]\n",
        "    response = ollama.chat(\n",
        "        model=MODEL_ID,\n",
        "        messages=messages,\n",
        "        options={\n",
        "            \"num_predict\": MAX_TOKENS,\n",
        "            \"temperature\": TEMPERATURE,\n",
        "            \"top_p\": TOP_P,\n",
        "        },\n",
        "    )\n",
        "    return (response.get(\"message\") or {}).get(\"content\", \"\") or \"\"\n",
        "\n",
        "\n",
        "LABEL_WORD_TO_ID = {\n",
        "    \"entailment\": 0,\n",
        "    \"neutral\": 1,\n",
        "    \"contradiction\": 2,\n",
        "    \"içerme\": 0,\n",
        "    \"tarafsız\": 1,\n",
        "    \"nötr\": 1,\n",
        "    \"çelişki\": 2,\n",
        "}\n",
        "\n",
        "\n",
        "def parse_generated_label(raw_text: str) -> int:\n",
        "    \"\"\"Extract first word from model output; strip punctuation; lowercase; map EN+TR; default 1 (neutral).\"\"\"\n",
        "    if not raw_text or not isinstance(raw_text, str):\n",
        "        return 1\n",
        "    text = raw_text.strip()\n",
        "    if not text:\n",
        "        return 1\n",
        "    # First token/word (split by whitespace)\n",
        "    parts = text.split()\n",
        "    first = parts[0] if parts else \"\"\n",
        "    # Strip punctuation and lowercase (handles quoted output e.g. 'entailment')\n",
        "    first = re.sub(r\"[.,;:!?\\\"'()\\[\\]]+\", \"\", first).strip().lower()\n",
        "    if not first:\n",
        "        return 1\n",
        "    return LABEL_WORD_TO_ID.get(first, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "run-inference",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_prompted_inference(ds):\n",
        "    premises = ds[\"premise\"]\n",
        "    hypotheses = ds[\"hypothesis\"]\n",
        "    labels = ds[\"label\"]\n",
        "    n = len(labels)\n",
        "    y_pred = []\n",
        "    debug_indices = set(list(range(min(5, n))) + list(range(0, n, 100)))\n",
        "\n",
        "    for start in tqdm(range(0, n, BATCH_SIZE), desc=\"Inference\"):\n",
        "        end = min(start + BATCH_SIZE, n)\n",
        "        for i in range(start, end):\n",
        "            user_content = nli_user_prompt(premises[i], hypotheses[i])\n",
        "            raw = ollama_chat_single(user_content)\n",
        "            label_id = parse_generated_label(raw)\n",
        "            y_pred.append(label_id)\n",
        "            if i in debug_indices:\n",
        "                print(f\"[sample {i}] raw: {repr(raw)} -> parsed: {label_id} ({LABEL_NAMES[label_id]})\")\n",
        "\n",
        "    y_true = np.array(labels, dtype=np.int64)\n",
        "    y_pred = np.array(y_pred, dtype=np.int64)\n",
        "    print(\"True label dist:\", dict(Counter(y_true)))\n",
        "    print(\"Pred label dist:\", dict(Counter(y_pred)))\n",
        "    return y_true, y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "metrics-plot",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics(y_true, y_pred):\n",
        "    acc = float(accuracy_score(y_true, y_pred))\n",
        "    f1_macro = float(f1_score(y_true, y_pred, average=\"macro\", zero_division=0))\n",
        "    f1_per_class = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
        "    f1_per_class = {LABEL_NAMES[i]: float(f1_per_class[i]) for i in range(NUM_LABELS)}\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    out = {\"accuracy\": acc, \"f1_macro\": f1_macro, \"f1_per_class\": f1_per_class}\n",
        "    return out, cm\n",
        "\n",
        "\n",
        "def save_confusion_plot(cm, path):\n",
        "    if not HAS_PLOT:\n",
        "        return\n",
        "    fig, ax = plt.subplots(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=LABEL_NAMES, yticklabels=LABEL_NAMES, ax=ax)\n",
        "    ax.set_xlabel(\"Predicted\")\n",
        "    ax.set_ylabel(\"True\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path)\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "main-loop",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating snli_tr_1_1 / test ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference:   0%|          | 0/1638 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sample 0] raw: 'neutral' -> parsed: 1 (neutral)\n",
            "[sample 1] raw: 'entailment' -> parsed: 0 (entailment)\n",
            "[sample 2] raw: 'entailment' -> parsed: 0 (entailment)\n",
            "[sample 3] raw: 'entailment' -> parsed: 0 (entailment)\n",
            "[sample 4] raw: 'entailment' -> parsed: 0 (entailment)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference:   1%|          | 16/1638 [00:19<33:34,  1.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sample 100] raw: 'neutral' -> parsed: 1 (neutral)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference:   2%|▏         | 33/1638 [00:40<32:57,  1.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sample 200] raw: 'neutral' -> parsed: 1 (neutral)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference:   3%|▎         | 50/1638 [01:02<33:50,  1.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sample 300] raw: 'entailment' -> parsed: 0 (entailment)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference:   4%|▍         | 66/1638 [01:22<33:54,  1.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sample 400] raw: 'neutral' -> parsed: 1 (neutral)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference:   5%|▌         | 83/1638 [01:44<33:02,  1.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sample 500] raw: 'entailment' -> parsed: 0 (entailment)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference:   6%|▌         | 100/1638 [02:07<34:39,  1.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sample 600] raw: 'entailment' -> parsed: 0 (entailment)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference:   7%|▋         | 116/1638 [02:29<36:41,  1.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sample 700] raw: 'neutral' -> parsed: 1 (neutral)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference:   8%|▊         | 133/1638 [02:56<42:50,  1.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sample 800] raw: 'entailment' -> parsed: 0 (entailment)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference:   9%|▉         | 150/1638 [03:29<46:21,  1.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sample 900] raw: 'neutral' -> parsed: 1 (neutral)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference:  10%|█         | 166/1638 [03:56<39:26,  1.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sample 1000] raw: 'neutral' -> parsed: 1 (neutral)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference:  11%|█         | 183/1638 [04:22<38:09,  1.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sample 1100] raw: 'entailment' -> parsed: 0 (entailment)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference:  12%|█▏        | 200/1638 [04:49<37:17,  1.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sample 1200] raw: 'entailment' -> parsed: 0 (entailment)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference:  13%|█▎        | 216/1638 [05:14<36:19,  1.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sample 1300] raw: 'neutral' -> parsed: 1 (neutral)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference:  14%|█▍        | 233/1638 [05:40<35:42,  1.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sample 1400] raw: 'neutral' -> parsed: 1 (neutral)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference:  15%|█▌        | 250/1638 [06:05<35:33,  1.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sample 1500] raw: 'entailment' -> parsed: 0 (entailment)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference:  16%|█▌        | 266/1638 [06:29<33:53,  1.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sample 1600] raw: 'entailment' -> parsed: 0 (entailment)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference:  17%|█▋        | 283/1638 [06:54<33:04,  1.46s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sample 1700] raw: 'entailment' -> parsed: 0 (entailment)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference:  18%|█▊        | 300/1638 [07:19<33:25,  1.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sample 1800] raw: 'entailment' -> parsed: 0 (entailment)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference:  19%|█▉        | 316/1638 [07:43<32:01,  1.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sample 1900] raw: 'neutral' -> parsed: 1 (neutral)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference:  20%|██        | 333/1638 [08:08<32:01,  1.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sample 2000] raw: 'entailment' -> parsed: 0 (entailment)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference:  21%|██▏       | 350/1638 [08:32<31:00,  1.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sample 2100] raw: 'entailment' -> parsed: 0 (entailment)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference:  22%|██▏       | 366/1638 [08:57<35:29,  1.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sample 2200] raw: 'entailment' -> parsed: 0 (entailment)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference:  23%|██▎       | 383/1638 [09:24<32:58,  1.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sample 2300] raw: 'entailment' -> parsed: 0 (entailment)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference:  24%|██▍       | 400/1638 [09:49<32:14,  1.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sample 2400] raw: 'entailment' -> parsed: 0 (entailment)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference:  25%|██▌       | 416/1638 [10:14<30:22,  1.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sample 2500] raw: 'entailment' -> parsed: 0 (entailment)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference:  26%|██▋       | 433/1638 [10:40<31:19,  1.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sample 2600] raw: 'neutral' -> parsed: 1 (neutral)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference:  27%|██▋       | 450/1638 [11:08<31:19,  1.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sample 2700] raw: 'neutral' -> parsed: 1 (neutral)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference:  28%|██▊       | 463/1638 [11:27<29:35,  1.51s/it]"
          ]
        }
      ],
      "source": [
        "Path(RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n",
        "all_metrics = {}\n",
        "\n",
        "for config_name in CONFIGS:\n",
        "    ds_dict = datasets[config_name]\n",
        "    split_names = EVAL_SPLITS[config_name]\n",
        "    all_metrics[config_name] = {}\n",
        "\n",
        "    for split_name in split_names:\n",
        "        if split_name not in ds_dict:\n",
        "            print(f\"  Skip {config_name}/{split_name} (missing)\")\n",
        "            continue\n",
        "        ds = ds_dict[split_name]\n",
        "        print(f\"Evaluating {config_name} / {split_name} ...\")\n",
        "        y_true, y_pred = run_prompted_inference(ds)\n",
        "        metrics, cm = compute_metrics(y_true, y_pred)\n",
        "        all_metrics[config_name][split_name] = metrics\n",
        "\n",
        "        cm_path = Path(RESULTS_DIR) / f\"confusion_{config_name}_{split_name}.csv\"\n",
        "        np.savetxt(cm_path, cm, fmt=\"%d\", delimiter=\",\")\n",
        "        save_confusion_plot(cm, Path(RESULTS_DIR) / f\"confusion_{config_name}_{split_name}.png\")\n",
        "\n",
        "        print(f\"  accuracy={metrics['accuracy']:.4f}, f1_macro={metrics['f1_macro']:.4f}\")\n",
        "\n",
        "with open(Path(RESULTS_DIR) / \"metrics.json\", \"w\") as f:\n",
        "    json.dump(all_metrics, f, indent=2)\n",
        "print(f\"Saved {RESULTS_DIR}/metrics.json\")\n",
        "sys.stdout = _nli_orig_stdout\n",
        "_nli_log_file.close()\n",
        "print(f\"Log closed: {_nli_log_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "summary",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary: per config/split\n",
        "for config_name, splits in all_metrics.items():\n",
        "    for split_name, m in splits.items():\n",
        "        print(f\"{config_name} / {split_name}: acc={m['accuracy']:.4f}, F1_macro={m['f1_macro']:.4f}, F1_per_class={m['f1_per_class']}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
