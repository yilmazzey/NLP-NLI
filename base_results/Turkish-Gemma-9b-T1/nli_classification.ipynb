{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "da877e2a",
      "metadata": {},
      "source": [
        "# NLI base results: Turkish-Gemma-9b-T1 (ytu-ce-cosmos/Turkish-Gemma-9b-T1)\n",
        "\n",
        "Loads [yilmazzey/sdp2-nli](https://huggingface.co/datasets/yilmazzey/sdp2-nli) (snli_tr_1_1, multinli_tr_1_1, trglue_mnli) and runs **test-only** evaluation with this model.\n",
        "\n",
        "9B generative LLM (Gemma-2 based, Turkish instruction-tuned with reasoning/thinking). Zero-shot prompted NLI evaluation (no fine-tuning). Expect variable but potentially strong performance due to Turkish adaptation. Outputs parsed to 0=entailment, 1=neutral, 2=contradiction.\n",
        "\n",
        "**Splits:** snli → test; multinli → validation_matched/mismatched; trglue → test_matched/test_mismatched. **Metrics:** Accuracy, macro F1, per-class F1, confusion matrix (CSV + plot)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "52e173bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "REPO_ID = \"yilmazzey/sdp2-nli\"\n",
        "CONFIGS = [\"snli_tr_1_1\", \"multinli_tr_1_1\", \"trglue_mnli\"]\n",
        "MODEL_ID = \"ytu-ce-cosmos/Turkish-Gemma-9b-T1\"\n",
        "NUM_LABELS = 3  # entailment, neutral, contradiction\n",
        "RESULTS_DIR = \"results\"\n",
        "# Lower to 8-16 if GPU memory low (9B model is heavy). If CPU only, expect very slow run.\n",
        "BATCH_SIZE = 32\n",
        "EVAL_SPLITS = {\n",
        "    \"snli_tr_1_1\": [\"test\"],\n",
        "    \"multinli_tr_1_1\": [\"validation_matched\", \"validation_mismatched\"],\n",
        "    \"trglue_mnli\": [\"test_matched\", \"test_mismatched\"],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0aeb6651",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/zeynep_yilmaz/Desktop/Turkish_NLI/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Colab: uncomment and run once to install/upgrade (Runtime -> Change runtime type -> GPU)\n",
        "# !pip install -q -U transformers datasets accelerate scikit-learn tqdm\n",
        "\n",
        "import json\n",
        "import random\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
        "from tqdm import tqdm\n",
        "from transformers import pipeline\n",
        "\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    HAS_PLOT = True\n",
        "except ImportError:\n",
        "    HAS_PLOT = False\n",
        "\n",
        "LABEL_NAMES = [\"entailment\", \"neutral\", \"contradiction\"]\n",
        "\n",
        "# Colab: confirm GPU (e.g. Tesla T4 / A100)\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"No GPU; 9B model will be very slow on CPU.\")\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c3d2e67c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading yilmazzey/sdp2-nli :: snli_tr_1_1 ...\n",
            "  splits: ['train', 'validation', 'test']\n",
            "Loading yilmazzey/sdp2-nli :: multinli_tr_1_1 ...\n",
            "  splits: ['train', 'validation_matched', 'validation_mismatched']\n",
            "Loading yilmazzey/sdp2-nli :: trglue_mnli ...\n",
            "  splits: ['train', 'validation_matched', 'validation_mismatched', 'test_matched', 'test_mismatched']\n"
          ]
        }
      ],
      "source": [
        "# Load all three dataset configs\n",
        "datasets = {}\n",
        "for cfg in CONFIGS:\n",
        "    print(f\"Loading {REPO_ID} :: {cfg} ...\")\n",
        "    datasets[cfg] = load_dataset(REPO_ID, cfg)\n",
        "    print(\"  splits:\", list(datasets[cfg].keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3812924f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model and tokenizer (9B params; first run may download ~18GB and take several minutes)...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|██████████| 4/4 [1:23:50<00:00, 1257.72s/it]  \n",
            "The following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Loading weights: 100%|██████████| 464/464 [00:20<00:00, 22.34it/s, Materializing param=model.norm.weight]                                \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded.\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading model and tokenizer (9B params; first run may download ~18GB and take several minutes)...\")\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=MODEL_ID,\n",
        "    device=0 if torch.cuda.is_available() else -1,\n",
        "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
        "    trust_remote_code=True,\n",
        "    model_kwargs={\"low_cpu_mem_usage\": True},\n",
        ")\n",
        "if hasattr(generator.tokenizer, \"pad_token\") and generator.tokenizer.pad_token is None:\n",
        "    generator.tokenizer.pad_token = generator.tokenizer.eos_token\n",
        "print(\"Model loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7a33fe84",
      "metadata": {},
      "outputs": [],
      "source": [
        "def nli_prompt(premise, hypothesis):\n",
        "    return f\"\"\"Premise: {premise}\n",
        "Hypothesis: {hypothesis}\n",
        "Does the premise entail, is neutral to, or contradict the hypothesis? Answer with only one word: entailment, neutral, or contradiction.\"\"\"\n",
        "\n",
        "\n",
        "LABEL_WORD_TO_ID = {\n",
        "    \"entailment\": 0,\n",
        "    \"neutral\": 1,\n",
        "    \"contradiction\": 2,\n",
        "    \"içerme\": 0,\n",
        "    \"tarafsız\": 1,\n",
        "    \"nötr\": 1,\n",
        "    \"çelişki\": 2,\n",
        "}\n",
        "\n",
        "\n",
        "def parse_generated_label(generated_text, prompt_text):\n",
        "    \"\"\"Take first word after prompt; map to 0=entailment, 1=neutral, 2=contradiction. Default 1 if unparseable.\"\"\"\n",
        "    continuation = generated_text\n",
        "    if generated_text.startswith(prompt_text):\n",
        "        continuation = generated_text[len(prompt_text):]\n",
        "    continuation = continuation.strip()\n",
        "    first_word = (continuation.split()[0].lower().rstrip(\".,;:\") if continuation else \"neutral\")\n",
        "    return LABEL_WORD_TO_ID.get(first_word, 1)\n",
        "\n",
        "\n",
        "def run_prompted_inference(ds):\n",
        "    premises = ds[\"premise\"]\n",
        "    hypotheses = ds[\"hypothesis\"]\n",
        "    labels = ds[\"label\"]\n",
        "    n = len(labels)\n",
        "    prompts = [nli_prompt(p, h) for p, h in zip(premises, hypotheses)]\n",
        "    y_pred = []\n",
        "    for start in tqdm(range(0, n, BATCH_SIZE), desc=\"Inference\"):\n",
        "        batch_prompts = prompts[start : start + BATCH_SIZE]\n",
        "        out = generator(\n",
        "            batch_prompts,\n",
        "            max_new_tokens=10,\n",
        "            do_sample=False,\n",
        "            pad_token_id=generator.tokenizer.pad_token_id or generator.tokenizer.eos_token_id,\n",
        "        )\n",
        "        for i, prompt_text in enumerate(batch_prompts):\n",
        "            gen_text = out[i][0][\"generated_text\"]\n",
        "            y_pred.append(parse_generated_label(gen_text, prompt_text))\n",
        "    y_true = list(labels)\n",
        "    y_true = np.array(y_true, dtype=np.int64)\n",
        "    y_pred = np.array(y_pred, dtype=np.int64)\n",
        "    print(\"True label dist:\", dict(Counter(y_true)))\n",
        "    print(\"Pred label dist:\", dict(Counter(y_pred)))\n",
        "    return y_true, y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c9364898",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics(y_true, y_pred):\n",
        "    acc = float(accuracy_score(y_true, y_pred))\n",
        "    f1_macro = float(f1_score(y_true, y_pred, average=\"macro\", zero_division=0))\n",
        "    f1_per_class = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
        "    f1_per_class = {LABEL_NAMES[i]: float(f1_per_class[i]) for i in range(NUM_LABELS)}\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    out = {\"accuracy\": acc, \"f1_macro\": f1_macro, \"f1_per_class\": f1_per_class}\n",
        "    return out, cm\n",
        "\n",
        "\n",
        "def save_confusion_plot(cm, path):\n",
        "    if not HAS_PLOT:\n",
        "        return\n",
        "    fig, ax = plt.subplots(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=LABEL_NAMES, yticklabels=LABEL_NAMES, ax=ax)\n",
        "    ax.set_xlabel(\"Predicted\")\n",
        "    ax.set_ylabel(\"True\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path)\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a582be6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating snli_tr_1_1 / test ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference:   0%|          | 0/307 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Passing `generation_config` together with generation-related arguments=({'do_sample', 'pad_token_id', 'max_new_tokens'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
            "Both `max_new_tokens` (=10) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=10) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=10) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        }
      ],
      "source": [
        "Path(RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n",
        "all_metrics = {}\n",
        "\n",
        "for config_name in CONFIGS:\n",
        "    ds_dict = datasets[config_name]\n",
        "    split_names = EVAL_SPLITS[config_name]\n",
        "    all_metrics[config_name] = {}\n",
        "\n",
        "    for split_name in split_names:\n",
        "        if split_name not in ds_dict:\n",
        "            print(f\"  Skip {config_name}/{split_name} (missing)\")\n",
        "            continue\n",
        "        ds = ds_dict[split_name]\n",
        "        print(f\"Evaluating {config_name} / {split_name} ...\")\n",
        "        y_true, y_pred = run_prompted_inference(ds)\n",
        "        metrics, cm = compute_metrics(y_true, y_pred)\n",
        "        all_metrics[config_name][split_name] = metrics\n",
        "\n",
        "        cm_path = Path(RESULTS_DIR) / f\"confusion_{config_name}_{split_name}.csv\"\n",
        "        np.savetxt(cm_path, cm, fmt=\"%d\", delimiter=\",\")\n",
        "        save_confusion_plot(cm, Path(RESULTS_DIR) / f\"confusion_{config_name}_{split_name}.png\")\n",
        "\n",
        "        print(f\"  accuracy={metrics['accuracy']:.4f}, f1_macro={metrics['f1_macro']:.4f}\")\n",
        "\n",
        "with open(Path(RESULTS_DIR) / \"metrics.json\", \"w\") as f:\n",
        "    json.dump(all_metrics, f, indent=2)\n",
        "print(f\"Saved {RESULTS_DIR}/metrics.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2837df88",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary: per config/split\n",
        "for config_name, splits in all_metrics.items():\n",
        "    for split_name, m in splits.items():\n",
        "        print(f\"{config_name} / {split_name}: acc={m['accuracy']:.4f}, F1_macro={m['f1_macro']:.4f}, F1_per_class={m['f1_per_class']}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
