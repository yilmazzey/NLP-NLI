{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "01c3672e",
      "metadata": {},
      "source": [
        "# NLI base results: ConvBERT Turkish Cased (dbmdz/convbert-base-turkish-cased)\n",
        "\n",
        "Loads [yilmazzey/sdp2-nli](https://huggingface.co/datasets/yilmazzey/sdp2-nli) (snli_tr_1_1, multinli_tr_1_1, trglue_mnli) and runs **test-only** evaluation with this model.\n",
        "\n",
        "**No prompts:** BERT NLI is sequence-pair classification (premise [SEP] hypothesis → label).\n",
        "\n",
        "**Splits:** Test only where available: snli → `test`; multinli → `validation_matched`/`validation_mismatched` (no test); trglue → `test_matched`/`test_mismatched`.\n",
        "\n",
        "**Metrics:** Accuracy, macro F1, per-class F1, confusion matrix (CSV + plot). Model is raw pretrained ConvBERT for Turkish (~110M params, cased); random classification head (~33% expected). Efficient alternative to standard BERT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "811f73d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "REPO_ID = \"yilmazzey/sdp2-nli\"\n",
        "CONFIGS = [\"snli_tr_1_1\", \"multinli_tr_1_1\", \"trglue_mnli\"]\n",
        "MODEL_ID = \"dbmdz/convbert-base-turkish-cased\"\n",
        "NUM_LABELS = 3  # entailment, neutral, contradiction\n",
        "RESULTS_DIR = \"results\"\n",
        "# Lower to 16 or 8 if CPU is slow\n",
        "BATCH_SIZE = 32\n",
        "EVAL_SPLITS = {\n",
        "    \"snli_tr_1_1\": [\"test\"],\n",
        "    \"multinli_tr_1_1\": [\"validation_matched\", \"validation_mismatched\"],\n",
        "    \"trglue_mnli\": [\"test_matched\", \"test_mismatched\"],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "92a7219d",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/zeynep_yilmaz/Desktop/Turkish_NLI/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding\n",
        "\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    HAS_PLOT = True\n",
        "except ImportError:\n",
        "    HAS_PLOT = False\n",
        "\n",
        "LABEL_NAMES = [\"entailment\", \"neutral\", \"contradiction\"]\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4c7dfbd6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading yilmazzey/sdp2-nli :: snli_tr_1_1 ...\n",
            "  splits: ['train', 'validation', 'test']\n",
            "Loading yilmazzey/sdp2-nli :: multinli_tr_1_1 ...\n",
            "  splits: ['train', 'validation_matched', 'validation_mismatched']\n",
            "Loading yilmazzey/sdp2-nli :: trglue_mnli ...\n",
            "  splits: ['train', 'validation_matched', 'validation_mismatched', 'test_matched', 'test_mismatched']\n"
          ]
        }
      ],
      "source": [
        "# Load all three dataset configs\n",
        "datasets = {}\n",
        "for cfg in CONFIGS:\n",
        "    print(f\"Loading {REPO_ID} :: {cfg} ...\")\n",
        "    datasets[cfg] = load_dataset(REPO_ID, cfg)\n",
        "    print(\"  splits:\", list(datasets[cfg].keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "945ae2c4",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading weights: 100%|██████████| 281/281 [00:00<00:00, 1982.20it/s, Materializing param=convbert.encoder.layer.11.output.dense.weight]                                \n",
            "\u001b[1mConvBertForSequenceClassification LOAD REPORT\u001b[0m from: dbmdz/convbert-base-turkish-cased\n",
            "Key                        | Status     | \n",
            "---------------------------+------------+-\n",
            "embeddings.position_ids    | UNEXPECTED | \n",
            "classifier.dense.weight    | MISSING    | \n",
            "classifier.dense.bias      | MISSING    | \n",
            "classifier.out_proj.bias   | MISSING    | \n",
            "classifier.out_proj.weight | MISSING    | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
            "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Model loaded successfully\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID, num_labels=NUM_LABELS, ignore_mismatched_sizes=True)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "print(f\"Using device: {device}\")\n",
        "print(\"Model loaded successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6f84a27b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_fn(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"premise\"],\n",
        "        examples[\"hypothesis\"],\n",
        "        truncation=True,\n",
        "        max_length=256,\n",
        "    )\n",
        "\n",
        "\n",
        "def run_inference(ds):\n",
        "    remove_cols = [c for c in ds.column_names if c != \"label\"]\n",
        "    ds = ds.map(\n",
        "        tokenize_fn,\n",
        "        batched=True,\n",
        "        remove_columns=remove_cols,\n",
        "        desc=\"Tokenize\",\n",
        "    )\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    def collate_fn(examples):\n",
        "        labels = torch.tensor([ex[\"label\"] for ex in examples])\n",
        "        batch = data_collator([{k: v for k, v in ex.items() if k != \"label\"} for ex in examples])\n",
        "        batch[\"labels\"] = labels\n",
        "        return batch\n",
        "\n",
        "    loader = torch.utils.data.DataLoader(ds, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "    preds_list, labels_list = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc=\"Inference\"):\n",
        "            out = model(\n",
        "                input_ids=batch[\"input_ids\"].to(device),\n",
        "                attention_mask=batch[\"attention_mask\"].to(device),\n",
        "            )\n",
        "            preds_list.append(out.logits.argmax(-1).cpu().numpy())\n",
        "            labels_list.append(batch[\"labels\"].numpy())\n",
        "    y_pred = np.concatenate(preds_list)\n",
        "    y_true = np.concatenate(labels_list)\n",
        "    return y_true, y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "15d439d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics(y_true, y_pred):\n",
        "    acc = float(accuracy_score(y_true, y_pred))\n",
        "    f1_macro = float(f1_score(y_true, y_pred, average=\"macro\", zero_division=0))\n",
        "    f1_per_class = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
        "    f1_per_class = {LABEL_NAMES[i]: float(f1_per_class[i]) for i in range(NUM_LABELS)}\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    out = {\"accuracy\": acc, \"f1_macro\": f1_macro, \"f1_per_class\": f1_per_class}\n",
        "    return out, cm\n",
        "\n",
        "\n",
        "def save_confusion_plot(cm, path):\n",
        "    if not HAS_PLOT:\n",
        "        return\n",
        "    fig, ax = plt.subplots(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=LABEL_NAMES, yticklabels=LABEL_NAMES, ax=ax)\n",
        "    ax.set_xlabel(\"Predicted\")\n",
        "    ax.set_ylabel(\"True\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path)\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b824da25",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating snli_tr_1_1 / test ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenize: 100%|██████████| 9824/9824 [00:00<00:00, 49033.48 examples/s]\n",
            "Inference: 100%|██████████| 307/307 [18:36<00:00,  3.64s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True label dist: {np.int64(1): 3219, np.int64(0): 3368, np.int64(2): 3237}\n",
            "Pred label dist: {np.int64(1): 9780, np.int64(2): 44}\n",
            "  accuracy=0.3278, f1_macro=0.1676\n",
            "Evaluating multinli_tr_1_1 / validation_matched ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenize: 100%|██████████| 9809/9809 [00:00<00:00, 45373.01 examples/s]\n",
            "Inference: 100%|██████████| 307/307 [29:58<00:00,  5.86s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True label dist: {np.int64(1): 3123, np.int64(2): 3211, np.int64(0): 3475}\n",
            "Pred label dist: {np.int64(1): 9713, np.int64(2): 94, np.int64(0): 2}\n",
            "  accuracy=0.3186, f1_macro=0.1678\n",
            "Evaluating multinli_tr_1_1 / validation_mismatched ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenize: 100%|██████████| 9825/9825 [00:00<00:00, 35019.22 examples/s]\n",
            "Inference: 100%|██████████| 308/308 [25:41<00:00,  5.00s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True label dist: {np.int64(2): 3240, np.int64(0): 3456, np.int64(1): 3129}\n",
            "Pred label dist: {np.int64(1): 9748, np.int64(2): 76, np.int64(0): 1}\n",
            "  accuracy=0.3187, f1_macro=0.1666\n",
            "Evaluating trglue_mnli / test_matched ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenize: 100%|██████████| 9008/9008 [00:00<00:00, 65797.36 examples/s]\n",
            "Inference: 100%|██████████| 282/282 [20:22<00:00,  4.34s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True label dist: {np.int64(1): 3138, np.int64(2): 2946, np.int64(0): 2924}\n",
            "Pred label dist: {np.int64(1): 8993, np.int64(2): 15}\n",
            "  accuracy=0.3485, f1_macro=0.1732\n",
            "Evaluating trglue_mnli / test_mismatched ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenize: 100%|██████████| 9217/9217 [00:00<00:00, 60841.93 examples/s]\n",
            "Inference: 100%|██████████| 289/289 [20:18<00:00,  4.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True label dist: {np.int64(1): 3043, np.int64(0): 3101, np.int64(2): 3073}\n",
            "Pred label dist: {np.int64(1): 9209, np.int64(2): 8}\n",
            "  accuracy=0.3302, f1_macro=0.1659\n",
            "Saved results/metrics.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "Path(RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n",
        "all_metrics = {}\n",
        "\n",
        "for config_name in CONFIGS:\n",
        "    ds_dict = datasets[config_name]\n",
        "    split_names = EVAL_SPLITS[config_name]\n",
        "    all_metrics[config_name] = {}\n",
        "\n",
        "    for split_name in split_names:\n",
        "        if split_name not in ds_dict:\n",
        "            print(f\"  Skip {config_name}/{split_name} (missing)\")\n",
        "            continue\n",
        "        ds = ds_dict[split_name]\n",
        "        print(f\"Evaluating {config_name} / {split_name} ...\")\n",
        "        y_true, y_pred = run_inference(ds)\n",
        "        print(\"True label dist:\", dict(Counter(y_true)))\n",
        "        print(\"Pred label dist:\", dict(Counter(y_pred)))\n",
        "        metrics, cm = compute_metrics(y_true, y_pred)\n",
        "        all_metrics[config_name][split_name] = metrics\n",
        "\n",
        "        cm_path = Path(RESULTS_DIR) / f\"confusion_{config_name}_{split_name}.csv\"\n",
        "        np.savetxt(cm_path, cm, fmt=\"%d\", delimiter=\",\")\n",
        "        save_confusion_plot(cm, Path(RESULTS_DIR) / f\"confusion_{config_name}_{split_name}.png\")\n",
        "\n",
        "        print(f\"  accuracy={metrics['accuracy']:.4f}, f1_macro={metrics['f1_macro']:.4f}\")\n",
        "\n",
        "with open(Path(RESULTS_DIR) / \"metrics.json\", \"w\") as f:\n",
        "    json.dump(all_metrics, f, indent=2)\n",
        "print(f\"Saved {RESULTS_DIR}/metrics.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b0eae194",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "snli_tr_1_1 / test: acc=0.3278, F1_macro=0.1676, F1_per_class={'entailment': 0.0, 'neutral': 0.4929609969997692, 'contradiction': 0.009753124047546479}\n",
            "multinli_tr_1_1 / validation_matched: acc=0.3186, F1_macro=0.1678, F1_per_class={'entailment': 0.0005752085130859936, 'neutral': 0.481146774696167, 'contradiction': 0.02178517397881997}\n",
            "multinli_tr_1_1 / validation_mismatched: acc=0.3187, F1_macro=0.1666, F1_per_class={'entailment': 0.0005785363031530228, 'neutral': 0.48163392094431934, 'contradiction': 0.017490952955367914}\n",
            "trglue_mnli / test_matched: acc=0.3485, F1_macro=0.1732, F1_per_class={'entailment': 0.0, 'neutral': 0.5168576374577528, 'contradiction': 0.002701789935832489}\n",
            "trglue_mnli / test_mismatched: acc=0.3302, F1_macro=0.1659, F1_per_class={'entailment': 0.0, 'neutral': 0.49640874959190334, 'contradiction': 0.0012982797792924375}\n"
          ]
        }
      ],
      "source": [
        "# Summary: per config/split\n",
        "for config_name, splits in all_metrics.items():\n",
        "    for split_name, m in splits.items():\n",
        "        print(f\"{config_name} / {split_name}: acc={m['accuracy']:.4f}, F1_macro={m['f1_macro']:.4f}, F1_per_class={m['f1_per_class']}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
