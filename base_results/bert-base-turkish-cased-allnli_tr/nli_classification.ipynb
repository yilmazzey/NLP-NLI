{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLI classification: BERTurk All-NLI-TR (emrecan/bert-base-turkish-cased-allnli_tr)\n",
    "\n",
    "Loads [yilmazzey/sdp2-nli](https://huggingface.co/datasets/yilmazzey/sdp2-nli) (snli_tr_1_1, multinli_tr_1_1, trglue_mnli) and runs classification. Model is pre-finetuned on All-NLI-TR; use as-is for zero-shot eval or further fine-tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_ID = \"yilmazzey/sdp2-nli\"\n",
    "CONFIGS = [\"snli_tr_1_1\", \"multinli_tr_1_1\", \"trglue_mnli\"]\n",
    "MODEL_ID = \"emrecan/bert-base-turkish-cased-allnli_tr\"\n",
    "NUM_LABELS = 3  # entailment, neutral, contradiction\n",
    "RESULTS_DIR = \"results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all three dataset configs\n",
    "datasets = {}\n",
    "for cfg in CONFIGS:\n",
    "    print(f\"Loading {REPO_ID} :: {cfg} ...\")\n",
    "    datasets[cfg] = load_dataset(REPO_ID, cfg)\n",
    "    print(\"  splits:\", list(datasets[cfg].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID, num_labels=NUM_LABELS)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_splits(ds_dict):\n",
    "    \"\"\"Return split names that are validation or test (for evaluation).\"\"\"\n",
    "    return [s for s in ds_dict.keys() if s != \"train\"]\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"premise\"],\n",
    "        examples[\"hypothesis\"],\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "def evaluate_split(ds):\n",
    "    ds = ds.map(\n",
    "        lambda ex: tokenize_fn(ex),\n",
    "        batched=True,\n",
    "        remove_columns=[c for c in ds.column_names if c != \"label\"],\n",
    "        desc=\"Tokenize\",\n",
    "    )\n",
    "    ds.set_format(\"torch\")\n",
    "    loader = torch.utils.data.DataLoader(ds, batch_size=32)\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            out = model(\n",
    "                input_ids=batch[\"input_ids\"].to(device),\n",
    "                attention_mask=batch[\"attention_mask\"].to(device),\n",
    "            )\n",
    "            preds.append(out.logits.argmax(-1).cpu())\n",
    "            labels.append(batch[\"label\"])\n",
    "    preds = torch.cat(preds)\n",
    "    labels = torch.cat(labels)\n",
    "    acc = (preds == labels).float().mean().item()\n",
    "    return acc, preds.numpy(), labels.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "all_metrics = {}\n",
    "\n",
    "for config_name, ds_dict in datasets.items():\n",
    "    all_metrics[config_name] = {}\n",
    "    for split_name in get_eval_splits(ds_dict):\n",
    "        acc, _, _ = evaluate_split(ds_dict[split_name])\n",
    "        all_metrics[config_name][split_name] = {\"accuracy\": acc}\n",
    "        print(f\"{config_name} / {split_name}: accuracy = {acc:.4f}\")\n",
    "\n",
    "import json\n",
    "with open(Path(RESULTS_DIR) / \"metrics.json\", \"w\") as f:\n",
    "    json.dump(all_metrics, f, indent=2)\n",
    "print(f\"Saved to {RESULTS_DIR}/metrics.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
