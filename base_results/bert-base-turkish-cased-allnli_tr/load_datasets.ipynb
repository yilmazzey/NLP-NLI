{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78969caa",
   "metadata": {},
   "source": [
    "# Load Turkish NLI Datasets\n",
    "\n",
    "This notebook loads three Turkish Natural Language Inference (NLI) datasets from the `yilmazzey/sdp2-nli` collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f49425",
   "metadata": {},
   "source": [
    "## Install Required Libraries\n",
    "\n",
    "Install the datasets library if not already available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63690d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (4.4.2)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from datasets) (3.20.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/denizcii/Library/Python/3.14/lib/python/site-packages (from datasets) (2.3.4)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /Users/denizcii/Library/Python/3.14/lib/python/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /Users/denizcii/Library/Python/3.14/lib/python/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from requests>=2.32.2->datasets) (2.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/denizcii/Library/Python/3.14/lib/python/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/denizcii/Library/Python/3.14/lib/python/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/denizcii/Library/Python/3.14/lib/python/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/denizcii/Library/Python/3.14/lib/python/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m26.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d6b164",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f95054e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b911a074",
   "metadata": {},
   "source": [
    "## Load MultiNLI Turkish Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13f7d238",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 392599/392599 [00:00<00:00, 3901649.71 examples/s]\n",
      "Generating validation_matched split: 100%|██████████| 9809/9809 [00:00<00:00, 3004156.84 examples/s]\n",
      "Generating validation_mismatched split: 100%|██████████| 9825/9825 [00:00<00:00, 3571592.72 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiNLI Turkish dataset loaded successfully!\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['annotator_labels', 'genre', 'pairID', 'promptID', 'translation_annotations', 'premise', 'hypothesis', 'label'],\n",
      "        num_rows: 392599\n",
      "    })\n",
      "    validation_matched: Dataset({\n",
      "        features: ['annotator_labels', 'genre', 'pairID', 'promptID', 'translation_annotations', 'premise', 'hypothesis', 'label'],\n",
      "        num_rows: 9809\n",
      "    })\n",
      "    validation_mismatched: Dataset({\n",
      "        features: ['annotator_labels', 'genre', 'pairID', 'promptID', 'translation_annotations', 'premise', 'hypothesis', 'label'],\n",
      "        num_rows: 9825\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 548487/548487 [00:00<00:00, 6129917.39 examples/s]\n",
      "Generating validation split: 100%|██████████| 9836/9836 [00:00<00:00, 3121608.21 examples/s]\n",
      "Generating test split: 100%|██████████| 9824/9824 [00:00<00:00, 1570545.91 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNLI Turkish dataset loaded successfully!\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['annotator_labels', 'captionID', 'pairID', 'translation_annotations', 'premise', 'hypothesis', 'label'],\n",
      "        num_rows: 548487\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['annotator_labels', 'captionID', 'pairID', 'translation_annotations', 'premise', 'hypothesis', 'label'],\n",
      "        num_rows: 9836\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['annotator_labels', 'captionID', 'pairID', 'translation_annotations', 'premise', 'hypothesis', 'label'],\n",
      "        num_rows: 9824\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 162788/162788 [00:00<00:00, 4231108.00 examples/s]\n",
      "Generating validation_matched split: 100%|██████████| 9050/9050 [00:00<00:00, 3310233.82 examples/s]\n",
      "Generating validation_mismatched split: 100%|██████████| 9200/9200 [00:00<00:00, 2349893.23 examples/s]\n",
      "Generating test_matched split: 100%|██████████| 9008/9008 [00:00<00:00, 3935655.25 examples/s]\n",
      "Generating test_mismatched split: 100%|██████████| 9217/9217 [00:00<00:00, 3832546.84 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrGLUE MNLI dataset loaded successfully!\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 162788\n",
      "    })\n",
      "    validation_matched: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 9050\n",
      "    })\n",
      "    validation_mismatched: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 9200\n",
      "    })\n",
      "    test_matched: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 9008\n",
      "    })\n",
      "    test_mismatched: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 9217\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cache_dir = \"/Users/denizcii/Downloads/sdp2-nli/datasets\"\n",
    "\n",
    "multinli_ds = load_dataset(\"yilmazzey/sdp2-nli\", \"multinli_tr_1_1\", cache_dir=cache_dir)\n",
    "print(\"MultiNLI Turkish dataset loaded successfully!\")\n",
    "print(multinli_ds)\n",
    "\n",
    "snli_ds = load_dataset(\"yilmazzey/sdp2-nli\", \"snli_tr_1_1\", cache_dir=cache_dir)\n",
    "print(\"SNLI Turkish dataset loaded successfully!\")\n",
    "print(snli_ds)\n",
    "\n",
    "trglue_ds = load_dataset(\"yilmazzey/sdp2-nli\", \"trglue_mnli\", cache_dir=cache_dir)\n",
    "print(\"TrGLUE MNLI dataset loaded successfully!\")\n",
    "print(trglue_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b2dfb7",
   "metadata": {},
   "source": [
    "## Explore Dataset Splits\n",
    "\n",
    "Check what splits are available in each dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3add46a",
   "metadata": {},
   "source": [
    "## Load Pre-trained Turkish NLI Model\n",
    "\n",
    "Load BERTurk model already fine-tuned on All-NLI-TR for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b922957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (4.57.3)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from transformers) (3.20.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/denizcii/Library/Python/3.14/lib/python/site-packages (from transformers) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/denizcii/Library/Python/3.14/lib/python/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from transformers) (2025.10.23)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from requests->transformers) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from requests->transformers) (2025.11.12)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m26.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b00674b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: emrecan/bert-base-turkish-cased-allnli_tr\n",
      "Number of labels: 3\n",
      "Label mapping: {0: 'entailment', 1: 'neutral', 2: 'contradiction'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_name = \"emrecan/bert-base-turkish-cased-allnli_tr\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Number of labels: {model.config.num_labels}\")\n",
    "print(f\"Label mapping: {model.config.id2label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647c4e50",
   "metadata": {},
   "source": [
    "## Zero-Shot Evaluation\n",
    "\n",
    "Evaluate the pre-trained model on the datasets without additional training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d2ad89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating MultiNLI-TR - validation_matched (9809 samples)...\n",
      "  Processing 0/9809...\n",
      "  Processing 500/9809...\n",
      "  Processing 1000/9809...\n",
      "  Processing 1500/9809...\n",
      "  Processing 2000/9809...\n",
      "  Processing 2500/9809...\n",
      "  Processing 3000/9809...\n",
      "  Processing 3500/9809...\n",
      "  Processing 4000/9809...\n",
      "  Processing 4500/9809...\n",
      "  Processing 5000/9809...\n",
      "  Processing 5500/9809...\n",
      "  Processing 6000/9809...\n",
      "  Processing 6500/9809...\n",
      "  Processing 7000/9809...\n",
      "  Processing 7500/9809...\n",
      "  Processing 8000/9809...\n",
      "  Processing 8500/9809...\n",
      "  Processing 9000/9809...\n",
      "  Processing 9500/9809...\n",
      "\n",
      "MultiNLI-TR - validation_matched Results:\n",
      "Accuracy: 0.7983\n",
      "F1 Macro: 0.7979\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   entailment       0.86      0.78      0.82      3475\n",
      "      neutral       0.74      0.78      0.76      3123\n",
      "contradiction       0.80      0.84      0.82      3211\n",
      "\n",
      "     accuracy                           0.80      9809\n",
      "    macro avg       0.80      0.80      0.80      9809\n",
      " weighted avg       0.80      0.80      0.80      9809\n",
      "\n",
      "\n",
      "Evaluating MultiNLI-TR - validation_mismatched (9825 samples)...\n",
      "  Processing 0/9825...\n",
      "  Processing 500/9825...\n",
      "  Processing 1000/9825...\n",
      "  Processing 1500/9825...\n",
      "  Processing 2000/9825...\n",
      "  Processing 2500/9825...\n",
      "  Processing 3000/9825...\n",
      "  Processing 3500/9825...\n",
      "  Processing 4000/9825...\n",
      "  Processing 4500/9825...\n",
      "  Processing 5000/9825...\n",
      "  Processing 5500/9825...\n",
      "  Processing 6000/9825...\n",
      "  Processing 6500/9825...\n",
      "  Processing 7000/9825...\n",
      "  Processing 7500/9825...\n",
      "  Processing 8000/9825...\n",
      "  Processing 8500/9825...\n",
      "  Processing 9000/9825...\n",
      "  Processing 9500/9825...\n",
      "\n",
      "MultiNLI-TR - validation_mismatched Results:\n",
      "Accuracy: 0.8059\n",
      "F1 Macro: 0.8051\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   entailment       0.84      0.81      0.82      3456\n",
      "      neutral       0.76      0.76      0.76      3129\n",
      "contradiction       0.82      0.84      0.83      3240\n",
      "\n",
      "     accuracy                           0.81      9825\n",
      "    macro avg       0.81      0.81      0.81      9825\n",
      " weighted avg       0.81      0.81      0.81      9825\n",
      "\n",
      "\n",
      "Evaluating SNLI-TR - test (9824 samples)...\n",
      "  Processing 0/9824...\n",
      "  Processing 500/9824...\n",
      "  Processing 1000/9824...\n",
      "  Processing 1500/9824...\n",
      "  Processing 2000/9824...\n",
      "  Processing 2500/9824...\n",
      "  Processing 3000/9824...\n",
      "  Processing 3500/9824...\n",
      "  Processing 4000/9824...\n",
      "  Processing 4500/9824...\n",
      "  Processing 5000/9824...\n",
      "  Processing 5500/9824...\n",
      "  Processing 6000/9824...\n",
      "  Processing 6500/9824...\n",
      "  Processing 7000/9824...\n",
      "  Processing 7500/9824...\n",
      "  Processing 8000/9824...\n",
      "  Processing 8500/9824...\n",
      "  Processing 9000/9824...\n",
      "  Processing 9500/9824...\n",
      "\n",
      "SNLI-TR - test Results:\n",
      "Accuracy: 0.8731\n",
      "F1 Macro: 0.8725\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   entailment       0.89      0.88      0.89      3368\n",
      "      neutral       0.85      0.82      0.84      3219\n",
      "contradiction       0.87      0.91      0.89      3237\n",
      "\n",
      "     accuracy                           0.87      9824\n",
      "    macro avg       0.87      0.87      0.87      9824\n",
      " weighted avg       0.87      0.87      0.87      9824\n",
      "\n",
      "\n",
      "Evaluating TrGLUE-MNLI - test_matched (9008 samples)...\n",
      "  Processing 0/9008...\n",
      "  Processing 500/9008...\n",
      "  Processing 1000/9008...\n",
      "  Processing 1500/9008...\n",
      "  Processing 2000/9008...\n",
      "  Processing 2500/9008...\n",
      "  Processing 3000/9008...\n",
      "  Processing 3500/9008...\n",
      "  Processing 4000/9008...\n",
      "  Processing 4500/9008...\n",
      "  Processing 5000/9008...\n",
      "  Processing 5500/9008...\n",
      "  Processing 6000/9008...\n",
      "  Processing 6500/9008...\n",
      "  Processing 7000/9008...\n",
      "  Processing 7500/9008...\n",
      "  Processing 8000/9008...\n",
      "  Processing 8500/9008...\n",
      "  Processing 9000/9008...\n",
      "\n",
      "TrGLUE-MNLI - test_matched Results:\n",
      "Accuracy: 0.7501\n",
      "F1 Macro: 0.7434\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   entailment       0.76      0.92      0.83      2924\n",
      "      neutral       0.85      0.53      0.65      3138\n",
      "contradiction       0.68      0.82      0.74      2946\n",
      "\n",
      "     accuracy                           0.75      9008\n",
      "    macro avg       0.77      0.76      0.74      9008\n",
      " weighted avg       0.77      0.75      0.74      9008\n",
      "\n",
      "\n",
      "Evaluating TrGLUE-MNLI - test_mismatched (9217 samples)...\n",
      "  Processing 0/9217...\n",
      "  Processing 500/9217...\n",
      "  Processing 1000/9217...\n",
      "  Processing 1500/9217...\n",
      "  Processing 2000/9217...\n",
      "  Processing 2500/9217...\n",
      "  Processing 3000/9217...\n",
      "  Processing 3500/9217...\n",
      "  Processing 4000/9217...\n",
      "  Processing 4500/9217...\n",
      "  Processing 5000/9217...\n",
      "  Processing 5500/9217...\n",
      "  Processing 6000/9217...\n",
      "  Processing 6500/9217...\n",
      "  Processing 7000/9217...\n",
      "  Processing 7500/9217...\n",
      "  Processing 8000/9217...\n",
      "  Processing 8500/9217...\n",
      "  Processing 9000/9217...\n",
      "\n",
      "TrGLUE-MNLI - test_mismatched Results:\n",
      "Accuracy: 0.8134\n",
      "F1 Macro: 0.8068\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   entailment       0.85      0.93      0.89      3101\n",
      "      neutral       0.89      0.60      0.72      3043\n",
      "contradiction       0.74      0.90      0.81      3073\n",
      "\n",
      "     accuracy                           0.81      9217\n",
      "    macro avg       0.83      0.81      0.81      9217\n",
      " weighted avg       0.83      0.81      0.81      9217\n",
      "\n",
      "\n",
      "======================================================================\n",
      "SUMMARY - F1 MACRO SCORES\n",
      "======================================================================\n",
      "MultiNLI-TR (matched)               | Accuracy: 0.7983 | F1 Macro: 0.7979\n",
      "MultiNLI-TR (mismatched)            | Accuracy: 0.8059 | F1 Macro: 0.8051\n",
      "SNLI-TR                             | Accuracy: 0.8731 | F1 Macro: 0.8725\n",
      "TrGLUE-MNLI (matched)               | Accuracy: 0.7501 | F1 Macro: 0.7434\n",
      "TrGLUE-MNLI (mismatched)            | Accuracy: 0.8134 | F1 Macro: 0.8068\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_nli(model, tokenizer, dataset, dataset_name, split_name=None):\n",
    "    \"\"\"Evaluate NLI model on a dataset\"\"\"\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Get evaluation split\n",
    "    if split_name:\n",
    "        eval_data = dataset[split_name]\n",
    "    elif 'test' in dataset:\n",
    "        eval_data = dataset['test']\n",
    "        split_name = 'test'\n",
    "    elif 'test_matched' in dataset:\n",
    "        eval_data = dataset['test_matched']\n",
    "        split_name = 'test_matched'\n",
    "    elif 'validation' in dataset:\n",
    "        eval_data = dataset['validation']\n",
    "        split_name = 'validation'\n",
    "    elif 'validation_matched' in dataset:\n",
    "        eval_data = dataset['validation_matched']\n",
    "        split_name = 'validation_matched'\n",
    "    else:\n",
    "        print(f\"No test/validation split found in {dataset_name}\")\n",
    "        return None, None\n",
    "    \n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    print(f\"\\nEvaluating {dataset_name} - {split_name} ({len(eval_data)} samples)...\")\n",
    "    \n",
    "    for i, example in enumerate(eval_data):\n",
    "        if i % 500 == 0:\n",
    "            print(f\"  Processing {i}/{len(eval_data)}...\")\n",
    "        \n",
    "        # Format input based on dataset structure\n",
    "        premise = example.get('premise', example.get('sentence1', ''))\n",
    "        hypothesis = example.get('hypothesis', example.get('sentence2', ''))\n",
    "        \n",
    "        inputs = tokenizer(premise, hypothesis, \n",
    "                          return_tensors=\"pt\", \n",
    "                          padding=True, \n",
    "                          truncation=True, \n",
    "                          max_length=512)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred = torch.argmax(outputs.logits, dim=-1).item()\n",
    "        \n",
    "        predictions.append(pred)\n",
    "        true_labels.append(example['label'])\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    f1_macro = f1_score(true_labels, predictions, average='macro')\n",
    "    \n",
    "    print(f\"\\n{dataset_name} - {split_name} Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Macro: {f1_macro:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(true_labels, predictions, \n",
    "                               target_names=['entailment', 'neutral', 'contradiction']))\n",
    "    \n",
    "    return accuracy, f1_macro\n",
    "\n",
    "# Evaluate on all datasets (full splits)\n",
    "results = {}\n",
    "\n",
    "# MultiNLI-TR: has validation_matched and validation_mismatched\n",
    "results['MultiNLI-TR (matched)'] = evaluate_nli(model, tokenizer, multinli_ds, \"MultiNLI-TR\", \n",
    "                                                 split_name='validation_matched')\n",
    "results['MultiNLI-TR (mismatched)'] = evaluate_nli(model, tokenizer, multinli_ds, \"MultiNLI-TR\", \n",
    "                                                    split_name='validation_mismatched')\n",
    "\n",
    "# SNLI-TR: has standard test split\n",
    "results['SNLI-TR'] = evaluate_nli(model, tokenizer, snli_ds, \"SNLI-TR\")\n",
    "\n",
    "# TrGLUE-MNLI: has test_matched and test_mismatched\n",
    "results['TrGLUE-MNLI (matched)'] = evaluate_nli(model, tokenizer, trglue_ds, \"TrGLUE-MNLI\", \n",
    "                                                 split_name='test_matched')\n",
    "results['TrGLUE-MNLI (mismatched)'] = evaluate_nli(model, tokenizer, trglue_ds, \"TrGLUE-MNLI\", \n",
    "                                                    split_name='test_mismatched')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY - F1 MACRO SCORES\")\n",
    "print(\"=\"*70)\n",
    "for dataset_name, (acc, f1) in results.items():\n",
    "    print(f\"{dataset_name:35} | Accuracy: {acc:.4f} | F1 Macro: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b610087d",
   "metadata": {},
   "source": [
    "## (Optional) Fine-tune on sdp2-nli\n",
    "\n",
    "Further fine-tune the model on your datasets for improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fe7162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 28695/28695 [00:03<00:00, 9048.28 examples/s]\n",
      "/var/folders/d2/h7gh1hbn07s7f4hw3bgpym100000gn/T/ipykernel_2237/1481071993.py:56: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning setup complete. Uncomment trainer.train() to start training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "# Combine datasets for training\n",
    "def prepare_dataset(examples):\n",
    "    \"\"\"Tokenize premise-hypothesis pairs\"\"\"\n",
    "    premise = examples.get('premise', examples.get('sentence1'))\n",
    "    hypothesis = examples.get('hypothesis', examples.get('sentence2'))\n",
    "    return tokenizer(premise, hypothesis, truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "# Prepare training data\n",
    "train_datasets = []\n",
    "if 'train' in multinli_ds:\n",
    "    train_datasets.append(multinli_ds['train'])\n",
    "if 'train' in snli_ds:\n",
    "    train_datasets.append(snli_ds['train'])\n",
    "if 'train' in trglue_ds:\n",
    "    train_datasets.append(trglue_ds['train'])\n",
    "\n",
    "# Combine and tokenize\n",
    "combined_train = concatenate_datasets(train_datasets)\n",
    "tokenized_train = combined_train.map(prepare_dataset, batched=True)\n",
    "\n",
    "# Prepare validation data\n",
    "eval_datasets = []\n",
    "if 'validation_matched' in multinli_ds:\n",
    "    eval_datasets.append(multinli_ds['validation_matched'])\n",
    "elif 'validation' in multinli_ds:\n",
    "    eval_datasets.append(multinli_ds['validation'])\n",
    "    \n",
    "if 'validation' in snli_ds:\n",
    "    eval_datasets.append(snli_ds['validation'])\n",
    "    \n",
    "if 'validation_matched' in trglue_ds:\n",
    "    eval_datasets.append(trglue_ds['validation_matched'])\n",
    "elif 'validation' in trglue_ds:\n",
    "    eval_datasets.append(trglue_ds['validation'])\n",
    "\n",
    "combined_eval = concatenate_datasets(eval_datasets)\n",
    "tokenized_eval = combined_eval.map(prepare_dataset, batched=True)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "# Uncomment to fine-tune\n",
    "# trainer.train()\n",
    "# trainer.save_model(\"./fine_tuned_model\")\n",
    "\n",
    "print(\"Fine-tuning setup complete. Uncomment trainer.train() to start training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693afddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available splits in each dataset:\n",
      "\n",
      "MultiNLI-TR: ['train', 'validation_matched', 'validation_mismatched']\n",
      "SNLI-TR: ['train', 'validation', 'test']\n",
      "TrGLUE-MNLI: ['train', 'validation_matched', 'validation_mismatched', 'test_matched', 'test_mismatched']\n"
     ]
    }
   ],
   "source": [
    "print(\"Available splits in each dataset:\\n\")\n",
    "print(f\"MultiNLI-TR: {list(multinli_ds.keys())}\")\n",
    "print(f\"SNLI-TR: {list(snli_ds.keys())}\")\n",
    "print(f\"TrGLUE-MNLI: {list(trglue_ds.keys())}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
